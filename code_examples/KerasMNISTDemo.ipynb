{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify handwritten digits with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from: [the MNIST dataset](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Download the MNIST dataset from Internet](#01)\n",
    "2. [Preprocessing the dataset](#02)\n",
    "3. [Softmax Regression](#03)\n",
    "4. [A small Convolutional Neural Network](#04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import requests\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"01\">1. Download the MNIST dataset from Internet </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've made the dataset into a zipped tar file. You'll have to download it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url,file):\n",
    "    # Streaming, so we can iterate over the response.\n",
    "    r = requests.get(url, stream=True)\n",
    "\n",
    "    # Total size in bytes.\n",
    "    total_size = int(r.headers.get('content-length', 0)); \n",
    "    block_size = 1024\n",
    "    wrote = 0 \n",
    "    with open(file, 'wb') as f:\n",
    "        for data in tqdm_notebook(r.iter_content(block_size), total=np.ceil(total_size//block_size) , unit='KB', unit_scale=True):\n",
    "            wrote = wrote  + len(data)\n",
    "            f.write(data)\n",
    "    if total_size != 0 and wrote != total_size:\n",
    "        print(\"ERROR, something went wrong\") \n",
    "\n",
    "url = \"https://github.com/chi-hung/PythonTutorial/raw/master/datasets/mnist.tar.gz\"\n",
    "file = \"mnist.tar.gz\"\n",
    "print('Retrieving the MNIST dataset...')\n",
    "download_file(url,file)\n",
    "print('Extracting the MNIST dataset...')\n",
    "tar = tarfile.open(file)\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "print('Completed fetching the MNIST dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 folders of images will be extracted from the downloaded tar file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"02\">2. Preprocessing the dataset</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filePathsGen(rootPath):\n",
    "    paths=[]\n",
    "    dirs=[]\n",
    "    for dirPath,dirNames,fileNames in os.walk(rootPath):\n",
    "        for fileName in fileNames:\n",
    "            fullPath=os.path.join(dirPath,fileName)\n",
    "            paths.append((int(dirPath[len(rootPath) ]),fullPath))\n",
    "        dirs.append(dirNames)\n",
    "    return dirs,paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs,paths=filePathsGen('mnist/') # load the image paths\n",
    "dfPath=pd.DataFrame(paths,columns=['class','path']) # save image paths as a Pandas DataFrame\n",
    "dfPath.head(5) # see the first 5 paths of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### How many digit classes & how many figures belong to each of the classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCountPerClass=dfPath.groupby('class').count()\n",
    "dfCountPerClass.rename(columns={'path':'amount of figures'},inplace=True)\n",
    "dfCountPerClass.plot(kind='bar',rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Split the image paths into train($70\\%$), val($15\\%$), test($15\\%$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=dfPath.sample(frac=0.7) # sample 70% data to be the train dataset\n",
    "test=dfPath.drop(train.index) # the rest 30% are now the test dataset\n",
    "\n",
    "# take 50% of the test dataset as the validation dataset\n",
    "val=test.sample(frac=1/2)\n",
    "test=test.drop(val.index)\n",
    "\n",
    "# let's check the length of the train, val and test dataset.\n",
    "print('number of all figures = {:10}.'.format(len(dfPath)))\n",
    "print('number of train figures= {:9}.'.format(len(train)))\n",
    "print('number of val figures= {:10}.'.format(len(val)))\n",
    "print('number of test figures= {:9}.'.format(len(test)))\n",
    "\n",
    "# let's take a look: plotting 3 figures from the train dataset\n",
    "for j in range(3):\n",
    "    img=plt.imread(train['path'].iloc[j])\n",
    "    plt.imshow(img,cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####  Load images into RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataLoad(dfPath):\n",
    "    paths=dfPath['path'].values\n",
    "    x=np.zeros((len(paths),28,28),dtype=np.float32 )\n",
    "\n",
    "    for j in range(len(paths)):\n",
    "        x[j,:,:]=plt.imread(paths[j])/255\n",
    "\n",
    "    y=dfPath['class'].values\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y=dataLoad(train)\n",
    "val_x,val_y=dataLoad(val)\n",
    "test_x,test_y=dataLoad(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: loading all images to RAM might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tensor shapes:\\n\")\n",
    "print('train:',train_x.shape,train_y.shape)\n",
    "print('val  :',val_x.shape,val_y.shape)\n",
    "print('test :',test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"03\">3. Softmax Regression</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onehot-encoding the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "train_y_onehot = np.float32( enc.fit_transform(train_y.reshape(-1,1)) \\\n",
    "                   .toarray() )\n",
    "\n",
    "val_y_onehot = np.float32( enc.fit_transform(val_y.reshape(-1,1)) \\\n",
    "                 .toarray() )\n",
    "\n",
    "test_y_onehot = np.float32( enc.fit_transform(test_y.reshape(-1,1)) \\\n",
    "                  .toarray() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28,28)))\n",
    "model.add(Dense(10, activation='softmax') )\n",
    "\n",
    "sgd=SGD(lr=0.2, momentum=0.0, decay=0.0)\n",
    "model.compile(optimizer='sgd',\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details about the constructed model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist=model.fit(train_x, train_y_onehot,\n",
    "               epochs=20, batch_size=128,\n",
    "               validation_data=(val_x,val_y_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the accuracy climbs during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['acc'],ms=5,marker='o',label='accuracy')\n",
    "plt.plot(hist.history['val_acc'],ms=5,marker='o',label='val accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll probably want to evaluate or save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss & accuracy (evaluated on the test dataset)\n",
    "score = model.evaluate(test_x, test_y_onehot, batch_size=128)\n",
    "print(\"LOSS (evaluated on the test dataset)=     {}\".format(score[0]))\n",
    "print(\"ACCURACY (evaluated on the test dataset)= {}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model architecture & weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('first_try.json', 'w') as jsOut:\n",
    "    json.dump(model.to_json(), jsOut)\n",
    "\n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved model architecture & weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('first_try.json', 'r') as jsIn:\n",
    "    model_architecture=json.load(jsIn)\n",
    "    \n",
    "model_new=model_from_json(model_architecture)\n",
    "model_new.load_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the classification report (see if the trained model works well on the test data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y=model.predict(test_x).argmax(axis=1)\n",
    "from sklearn.metrics import classification_report\n",
    "print( classification_report(test_y,pred_y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"04\">4. A small Convolutional Neural Network</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the tensors (this step is necessary, because the CNN model wants the input tensor to be 4D):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.expand_dims(train_x,axis=-1)\n",
    "val_x = np.expand_dims(val_x,axis=-1)\n",
    "test_x = np.expand_dims(test_x,axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten,Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "in_shape=(28,28,1)\n",
    "# ========== BEGIN TO CREATE THE MODEL ==========\n",
    "model = Sequential()\n",
    "# feature extraction (2 conv layers)\n",
    "model.add(Conv2D(32, (3,3),\n",
    "                 activation='relu',\n",
    "                 input_shape=in_shape))\n",
    "model.add(Conv2D(64, (3,3), activation='relu')\n",
    "         )\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))\n",
    "         )\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "# classification (2 dense layers)\n",
    "model.add(Dense(128, activation='relu')\n",
    "         )\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# ========== COMPLETED THE MODEL CREATION========\n",
    "\n",
    "# Compile the model before training.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=0.01,momentum=0.1),\n",
    "              metrics=['accuracy'],\n",
    "              context=['gpu(0)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "hist=model.fit(train_x, train_y_onehot, \n",
    "               epochs=20,\n",
    "               batch_size=32,\n",
    "               validation_data=(val_x,val_y_onehot),\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the accuracy climbs during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['acc'],ms=5,marker='o',label='accuracy')\n",
    "plt.plot(hist.history['val_acc'],ms=5,marker='o',label='val accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the classification report (see if the trained model works well on the test data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y=model.predict(test_x).argmax(axis=1)\n",
    "from sklearn.metrics import classification_report\n",
    "print( classification_report(test_y,pred_y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
